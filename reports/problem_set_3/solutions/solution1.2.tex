\subsection{}
The likelihood is given by

\begin{equation*}
    P(Y|\gamma) = \Pi_{i=1}^n \frac{\gamma^y e^{-\gamma}}{y!},
\end{equation*}

and the prior is given by

\begin{equation*}
    f(\lambda ) = \frac{\beta^\alpha}{\Gamma(\alpha)}\gamma^{\alpha - 1} \e^{-\beta x}.
\end{equation*}

Posterior:

\begin{align*}
    P(\lambda | Y_1, \ldots, Y_n) &= \frac{P(Y-1, \ldots, Y_n | \gamma) P(\gamma)}{P(Y_1, \ldots, Y_n)} \\
                                 & \propto \Pi_{i=1}^{n} \frac{\gamma^y e^{-\gamma}}{y!} \left( \frac{\beta^\alpha}{\Gamma(\alpha)}\gamma^{\alpha - 1} \e^{-\beta x} \right) \\
                                 & \propto \frac{\gamma^{\sum y_i e^{-n\gamma}}}{\Pi_{i=1}^n (y_i!)} \left( \frac{\beta^\alpha}{\Gamma(\alpha)}\gamma^{\alpha - 1} \e^{-\beta x} \right) \\
                                 & \propto \gamma^{\sum y_i + \alpha - 1} e^{-(n + \beta) x}
\end{align*}        

where $\Pi_{i=1}^n(y_i!)$ and $\Gamma(\alpha)$ act as normalizations. This can be expressed as the gamma function:

\begin{equation*}
    P(\gamma|Y_1, \ldots, Y_n) = \texttt{gamma} (\sum y_i + \alpha, n + \beta).
\end{equation*}


\begin{minted}{python}
def exact_posterior(Y, alpha, beta):

    N = len(Y)

    # define closed-form posterior
    a = np.sum(Y) + alpha
    scale = 1 / (N + beta)
    
    return sps.gamma(a, scale=scale)


SAMPLES = 5000
lambda_space = np.linspace(11, 15)

posterior = exact_posterior(Y, alpha, beta)
exact_posterior_density = posterior.pdf(lambda_space)
exact_samples = posterior.rvs(SAMPLES)
\end{minted}
